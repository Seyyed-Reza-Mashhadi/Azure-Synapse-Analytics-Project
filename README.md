<img width="1919" height="500" alt="banner" src="https://github.com/user-attachments/assets/8e49e8dd-418d-418c-bbbb-e9ff81318cdc" />

# ‚òÅÔ∏è Azure Synapse Analytics Project  

This project demonstrates how to perform analytics directly in **Azure Synapse Analytics**, leveraging different query engines and compute options. Using both the cleaned Parquet data produced by the Azure Data Factory project and the raw CSV files, the focus is on showcasing various Synapse tools for querying, modeling, and analyzing data, including:  
- **Serverless SQL Pools** (on-demand queries)  
- **Dedicated SQL Pools** (provisioned compute for structured analytics)  
- **Apache Spark Pools** (data exploration, transformation, and visualization)  

üîó **Dataset:** The data is available on [Kaggle](https://www.kaggle.com/datasets/155a87ba8d7e92c5896ddc7f3ca3e3fa9c799207ed8dbf9a1cedf2e2e03e3c14). Raw CSV files and processed Parquet outputs were already available in ADLS, prepared via pipelines from the [Azure Data Factory Project](https://github.com/Seyyed-Reza-Mashhadi/Azure-Data-Factory-Project).  


## üéØ Project Goals  

- Showcase how **different Synapse compute engines** (Serverless SQL, Dedicated SQL, Spark) can be applied to the same dataset  
- Demonstrate **end-to-end analytics workflow**: raw data exploration ‚Üí star schema modeling ‚Üí BI readiness  
- Highlight **cost-efficient data analysis strategies** in Synapse (when to use serverless vs. dedicated vs. Spark)  
- Enable **seamless Power BI integration** with Synapse for interactive dashboards  
- Provide a **reusable reference architecture** for modern data analytics on Azure  


# ‚öôÔ∏è Step-by-Step Implementation  

## 1Ô∏è‚É£ Azure Resources  

Provisioned within the same resource group:  
- **Azure Data Lake Storage Gen2** ‚Üí contains raw CSVs + cleaned Parquet outputs  
- **Azure Synapse Analytics** (workspace) with:  
  - **Serverless SQL Pool** ‚Üí default, on-demand queries  
  - **Dedicated SQL Pool** ‚Üí provisioned compute, relational schema  
  - **Apache Spark Pool** ‚Üí notebooks for data exploration & visualization  

<p align="center">
  <img src="https://github.com/user-attachments/assets/7ecaa9df-effd-4269-919a-13101035960a" width="350">
</p>  

## 2Ô∏è‚É£ Serverless SQL Pool  

Serverless SQL is Synapse‚Äôs **on-demand query engine** that lets you run SQL queries directly on files in **ADLS Gen2** without loading them into a database. It is ideal for **quick ad-hoc analysis** and validation of transformed outputs. Serverless SQL Pool features include:  
- No infrastructure provisioning required  
- Ideal for exploration & validation of raw/processed data  
- Can query diverse file formats (CSV, Parquet, JSON) directly  
- **Cost Model:** pay only per query (cost-efficient)  
  - ~$5 per TB of data processed  
  - Minimum 10 MB per query (even for smaller files ‚Üí still very cheap)  

### **Query Raw Data**

In this project, two example for raw data queries are illustrated using serverless SQL pool:  

- Queried Parquet files directly from ADLS Gen2 to get the top 5 most expensive "durable" products [[View SQL File](https://github.com/Seyyed-Reza-Mashhadi/Azure-Synapse-Analytics-Project/blob/main/SQL%20files/0_Direct_Query_csv.sql)]  

<p align="center">
  <img src="https://github.com/user-attachments/assets/fd8d104b-817b-420d-b15a-4832bf08480d" width="700">
</p>  

- Queried raw CSV directly from ADLS Gen2 to get the total number of cities with sales [[View SQL File](https://github.com/Seyyed-Reza-Mashhadi/Azure-Synapse-Analytics-Project/blob/main/SQL%20files/0_Direct_Query_parquet.sql)]    

<p align="center">
  <img src="https://github.com/user-attachments/assets/e27d7e2f-797e-468c-b93c-fa3edffbab8d" width="700">
</p>  



### **Query External Tables**

Serverless SQL pool also supports **external tables**, which are metadata definitions inside the database that point to files in ADLS.  
Key points about external tables:  
- The data itself remains in ADLS; the table only stores schema and location metadata  
- Allows querying raw or processed data as if it were a normal table in SQL  
- Useful for consistent queries, schema enforcement, and integration with views or downstream pipelines  
- Tables in serverless SQL pool are external by default (this is the opposite of dedicated SQL pool where tables are internal and data is physically stored in the database)  

Here is an example showing a query result using the serverless SQL pool for calculating the total revenue generated by different product categories. 


<p align="center">
  <img src="https://github.com/user-attachments/assets/5aa2a39d-8212-498b-98a2-2e4a25a70bb0" width="700">
</p>  

**‚ö°Quick Visualization in Synapse:**

Synapse provides some basic plotting options (bar charts, line charts, pie charts, etc.) that can be used for quick visualization of query results. These are useful for rapid validation and exploration, though more advanced analysis and dashboards are typically built in Power BI or using Apache Spark notebooks.


## 3Ô∏è‚É£ Dedicated SQL Pool  

Dedicated SQL Pool is **Synapse‚Äôs provisioned data warehouse** for structured, high-performance analytics using **T-SQL**. Dedicated SQL Pool features include:  
- Scalable data warehouse for BI workloads 
- Schema-aware integration with Power BI 
- High-performance, repeatable queries  
- Tables are **internal tables** (physically stored in the dedicated SQL database)  
- **Distribution Strategies:** Tables can use **hash, round-robin, or replicated distribution** to optimize parallel processing and query performance  
- **Cost Model:** Billed per hour based on **DWUs (Data Warehouse Units)**  
  - Compute billing stops when paused  
  - Storage billed at ~$0.12/GB/month  

In this project, processed **Parquet files** were loaded into the dedicated SQL database using **bulk load**. To optimize performance, the tables in the database were created with **Round-robin distribution** (ensuring even data spread across compute nodes), and a **Clustered Columnstore Index (CCI)** was applied by default, providing high compression and efficient analytical query execution on large datasets. After loading the data into internal tables, **Primary Keys** were introduced as **metadata** to provide schema clarity for downstream analysis (e.g., Power BI), without enforcing strict constraints, since Synapse Dedicated Pools are designed for **parallel, high-speed analytics**, not strict relational enforcement.

<br>
<p align="center">
  <i>Example: Loading data from FactSales.parquet file into the Dedicated SQL Pool table</i>
</p>

```sql
IF NOT EXISTS (SELECT * FROM sys.objects O JOIN sys.schemas S ON O.schema_id = S.schema_id WHERE O.NAME = 'FactSales' AND O.TYPE = 'U' AND S.NAME = 'dbo')
CREATE TABLE dbo.FactSales ([SalesID] int, [EmployeeID] int, [CustomerID] int, [ProductID] int,
	 [Quantity] smallint, [Discount] float, [TotalPrice] float, [SalesDate] datetime2(7), [TransactionNumber] nvarchar(4000))
WITH
	(DISTRIBUTION = ROUND_ROBIN,
	 CLUSTERED COLUMNSTORE INDEX
	 -- HEAP
	)
GO
--CREATE PROC bulk_load_FactSales
--AS
--BEGIN
COPY INTO dbo.FactSales
(SalesID 1, EmployeeID 2, CustomerID 3, ProductID 4, Quantity 5, Discount 6, TotalPrice 7, SalesDate 8, TransactionNumber 9)
FROM 'https://adlsgrocery.dfs.core.windows.net/processeddata/FactSales.parquet'
WITH (FILE_TYPE = 'PARQUET', MAXERRORS = 0 ,CREDENTIAL = ( IDENTITY = 'Managed Identity' ))
--END
GO
```

<br>

**Query Example**
- A query to get the top 5 best selling products (i.e., with highest generated revenue) [[View SQL File](https://github.com/Seyyed-Reza-Mashhadi/Azure-Synapse-Analytics-Project/blob/main/SQL%20files/2_Dedicated_Query_2.sql)]

<br>

<p align="center">
  <img src="https://github.com/user-attachments/assets/e1296dd9-35a1-464a-85e1-d78325dfec55" width="700">
</p>  


## 4Ô∏è‚É£ Apache Spark Pool  

Apache Spark in Synapse enables **data exploration, transformation, and visualization** using **Python / PySpark / ML libraries**. It is ideal for flexible analytics and exploratory data science workloads. Apache Spark features include:  
- Flexible data manipulation with **Python / PySpark / ML libraries**  
- Native integration with Synapse workspace for analytics & visualization  
- Suitable for **exploratory data science workloads**  
- Supports reading/writing from **ADLS Gen2**, **Dedicated SQL Pool**, or other external sources
- Spark notebooks can also be **integrated into Synapse pipelines** for scheduled or automated data processing, similar to Azure Data Factory pipelines.
- **Cost Model:** Billed per vCore-hour while the pool is running  
  - Must stop pool to avoid charges when not in use  
  - No persistent storage cost unless explicitly writing outputs  

**Example**
In this project, a histogram of customer total spendings is created using PySpark notebook ([View Notebook](https://github.com/Seyyed-Reza-Mashhadi/Azure-Synapse-Analytics-Project/blob/main/Notebook_CustomersSpending.ipynb)) following the steps below:  
- Read `FactSales` and `DimCustomers` parquet files from ADLS container.  
- Perform proper joins & aggregations (e.g., total spending per customer) and store results in a dataframe.  
- Calculate the median value of customer spending.  
- Generate the histogram of customer spendings using matplotlib.pyplot, plotting the median as a line.

<p align="center">
  <img src="https://github.com/user-attachments/assets/45d2c131-1e6c-467b-afca-6940970d5ba8" width="900">
</p>  


## 5Ô∏è‚É£ Connection to Power BI  

Two ways of connecting Synapse with Power BI:  
1. **Power BI Desktop** ‚Üí Connect using server name + credentials  
2. **Power BI Service (Online)** ‚Üí Link Synapse workspace as a **Linked Service** for managed connectivity  

Validated schema recognition and confirmed readiness for **dashboarding & BI analysis**.  



# üîÑ Synapse vs. ADF  

| Feature              | **Azure Data Factory (ADF)** | **Azure Synapse Analytics** |
|----------------------|-------------------------------|-----------------------------|
| **Primary Focus**    | ETL/ELT (data movement & transformation) | Analytics & data warehousing |
| **Data Handling**    | Ingest, clean, transform data | Query, model, visualize data |
| **Core Strengths**   | Pipelines, data orchestration | Serverless queries, Spark, Dedicated SQL pools |
| **Integration**      | Data prep for downstream use | BI-ready datasets & analysis |
| **Best For**         | Preparing structured data | Exploring & analyzing data |

üëâ In short: **ADF = Data Preparation**, **Synapse = Data Analysis**  

---

# üí∞ Cost Considerations  

- **Serverless SQL Pool** ‚Üí ~$5/TB scanned (min 10 MB/query)  
- **Dedicated SQL Pool** ‚Üí Pay for DWUs (pause to save costs, storage billed separately)  
- **Apache Spark Pool** ‚Üí Pay for vCore-hours (stop after use)  
- **ADLS Storage** ‚Üí ~$0.12/GB/month for persisted data  
- ‚ö†Ô∏è **Best Practices:**  
  - Pause/stop compute resources when idle  
  - Partition files for more efficient queries  
  - Use Parquet/Delta instead of CSV for cost + performance  


# üîë Key Features  

- Queried data directly from ADLS without ETL (Serverless SQL)  
- Built star schema in Dedicated SQL Pool  
- Integrated schema with Power BI for BI readiness  
- Used Apache Spark for aggregations & visualizations  
- Compared Synapse vs. ADF for clear separation of concerns  
- Highlighted cost efficiency across compute options  


# üîÅ Related Projects  

üìä **Azure Data Factory Project** ‚Üí End-to-end ETL pipelines & star schema prep  
üìä **SQL Project** ‚Üí PostgreSQL analytics on Grocery Sales dataset  
üìä **Power BI Dashboard** ‚Üí Interactive visual exploration of sales & customer insights  
