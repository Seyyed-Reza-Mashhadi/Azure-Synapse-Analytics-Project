<img width="1919" height="500" alt="banner" src="https://github.com/user-attachments/assets/8e49e8dd-418d-418c-bbbb-e9ff81318cdc" />

# ‚òÅÔ∏è Azure Synapse Analytics Project  

This project demonstrates how to perform analytics directly in **Azure Synapse Analytics**, leveraging different query engines and compute options. Using both the cleaned Parquet data produced by the Azure Data Factory project and the raw CSV files, the focus is on showcasing various Synapse tools for querying, modeling, and analyzing data, including:  
- **Serverless SQL Pools** (on-demand queries)  
- **Dedicated SQL Pools** (provisioned compute for structured analytics)  
- **Apache Spark Pools** (data exploration, transformation, and visualization)  

üîó **Dataset:** The data is available on [Kaggle](https://www.kaggle.com/datasets/155a87ba8d7e92c5896ddc7f3ca3e3fa9c799207ed8dbf9a1cedf2e2e03e3c14). Raw CSV files and processed Parquet outputs were already available in ADLS, prepared via pipelines from the [Azure Data Factory Project](https://github.com/Seyyed-Reza-Mashhadi/Azure-Data-Factory-Project).  


## üéØ Project Goals  

- Showcase how **different Synapse compute engines** (Serverless SQL, Dedicated SQL, Spark) can be applied to the same dataset  
- Demonstrate **end-to-end analytics workflow**: raw data exploration ‚Üí star schema modeling ‚Üí BI readiness  
- Highlight **cost-efficient data analysis strategies** in Synapse (when to use serverless vs. dedicated vs. Spark)  
- Enable **seamless Power BI integration** with Synapse for interactive dashboards  
- Provide a **reusable reference architecture** for modern data analytics on Azure  


# ‚öôÔ∏è Step-by-Step Implementation  

## 1Ô∏è‚É£ Azure Resources  

Provisioned within the same resource group:  
- **Azure Data Lake Storage Gen2** ‚Üí contains raw CSVs + cleaned Parquet outputs  
- **Azure Synapse Analytics** (workspace) with:  
  - **Serverless SQL Pool** ‚Üí default, on-demand queries  
  - **Dedicated SQL Pool** ‚Üí provisioned compute, relational schema  
  - **Apache Spark Pool** ‚Üí notebooks for data exploration & visualization  

<p align="center">
  <img src="https://github.com/user-attachments/assets/7ecaa9df-effd-4269-919a-13101035960a" width="350">
</p>  

## 2Ô∏è‚É£ Serverless SQL Pool  

Serverless SQL is Synapse‚Äôs **on-demand query engine** that lets you run SQL queries directly on files in **ADLS Gen2** without loading them into a database. It is ideal for **quick ad-hoc analysis** and validation of transformed outputs. Serverless SQL Pool features include:  
- No infrastructure provisioning required  
- Ideal for exploration & validation of raw/processed data  
- Can query diverse file formats (CSV, Parquet, JSON) directly  
- **Cost Model:** pay only per query (cost-efficient)  
  - ~$5 per TB of data processed  
  - Minimum 10 MB per query (even for smaller files ‚Üí still very cheap)  

### **Query Raw Data**

In this project, two example queries for raw data are illustrated using serverless SQL pool:  

- Queried Parquet files directly from ADLS Gen2 to get the top 5 most expensive "durable" products [[View SQL File](https://github.com/Seyyed-Reza-Mashhadi/Azure-Synapse-Analytics-Project/blob/main/SQL%20files/0_Direct_Query_csv.sql)]  

<p align="center">
  <img src="https://github.com/user-attachments/assets/fd8d104b-817b-420d-b15a-4832bf08480d" width="700">
</p>  

- Queried raw CSV directly from ADLS Gen2 to get the total number of cities with sales [[View SQL File](https://github.com/Seyyed-Reza-Mashhadi/Azure-Synapse-Analytics-Project/blob/main/SQL%20files/0_Direct_Query_parquet.sql)]    

<p align="center">
  <img src="https://github.com/user-attachments/assets/e27d7e2f-797e-468c-b93c-fa3edffbab8d" width="700">
</p>  


### **Query External Tables**

Serverless SQL pool also supports **external tables**, which are metadata definitions inside the database that point to files in ADLS.  
Key points about external tables:  
- The data itself remains in ADLS; the table only stores schema and location metadata  
- Allows querying raw or processed data as if it were a normal table in SQL  
- Useful for consistent queries, schema enforcement, and integration with views or downstream pipelines  
- Tables in serverless SQL pool are external by default (this is the opposite of dedicated SQL pool where tables are internal and data is physically stored in the database)  

Here is an example showing a query result using the serverless SQL pool for calculating the total revenue generated by different product categories [[View SQL File](https://github.com/Seyyed-Reza-Mashhadi/Azure-Synapse-Analytics-Project/blob/main/SQL%20files/1_Serverless_Query_1.sql)]. 

<p align="center">
  <img src="https://github.com/user-attachments/assets/5aa2a39d-8212-498b-98a2-2e4a25a70bb0" width="700">
</p>  

**‚ö°Quick Visualization in Synapse:**

Synapse provides some basic plotting options (bar charts, line charts, pie charts, etc.) that can be used for quick visualization of query results. These are useful for rapid validation and exploration, though more advanced analysis and dashboards are typically built in Power BI or using Apache Spark notebooks.


## 3Ô∏è‚É£ Dedicated SQL Pool  

Dedicated SQL Pool is **Synapse‚Äôs provisioned data warehouse** for structured, high-performance analytics using **T-SQL**. Dedicated SQL Pool features include:  
- Scalable data warehouse for BI workloads 
- Schema-aware integration with Power BI 
- High-performance, repeatable queries  
- Tables are **internal tables** (physically stored in the dedicated SQL database)  
- **Distribution Strategies:** Tables can use **hash, round-robin, or replicated distribution** to optimize parallel processing and query performance  
- **Cost Model:** Billed per hour based on **DWUs (Data Warehouse Units)**  
  - Compute billing stops when paused  
  - Storage billed at ~$0.12/GB/month  

In this project, processed **Parquet files** were loaded into the dedicated SQL database using **bulk load**. To optimize performance, the tables in the database were created with **Round-robin distribution** (ensuring even data spread across compute nodes), and a **Clustered Columnstore Index (CCI)** was applied by default, providing high compression and efficient analytical query execution on large datasets. **Primary Keys** were added as **metadata** after loading, to provide schema clarity for downstream analysis (e.g., Power BI), without enforcing strict constraints, since Synapse Dedicated Pools are designed for **parallel, high-speed analytics**, not strict relational enforcement.

**Query Example**
- Example of a T-SQL query to get the top 5 best-selling products (i.e., products with highest generated revenue) [[View SQL File](https://github.com/Seyyed-Reza-Mashhadi/Azure-Synapse-Analytics-Project/blob/main/SQL%20files/2_Dedicated_Query_2.sql)]

<br>

<p align="center">
  <img src="https://github.com/user-attachments/assets/e1296dd9-35a1-464a-85e1-d78325dfec55" width="700">
</p>  


## 4Ô∏è‚É£ Apache Spark Pool  

Apache Spark in Synapse enables **data exploration, transformation, and visualization** using **Python / PySpark / ML libraries**. It is ideal for flexible analytics and exploratory data science workloads. Apache Spark features include:  
- Flexible data manipulation with **Python / PySpark / ML libraries**  
- Native integration with Synapse workspace for analytics & visualization  
- Suitable for **exploratory data science workloads**  
- Supports reading/writing from **ADLS Gen2**, **Dedicated SQL Pool**, or other external sources
- Spark notebooks can also be **integrated into Synapse pipelines** for scheduled or automated data processing, similar to Azure Data Factory pipelines.
- **Cost Model:** Billed per vCore-hour while the pool is running  
  - Must stop pool to avoid charges when not in use  
  - No persistent storage cost unless explicitly writing outputs  

**Example**
In this project, the histogram of customer total spendings is created using PySpark notebook [[View Notebook](https://github.com/Seyyed-Reza-Mashhadi/Azure-Synapse-Analytics-Project/blob/main/Notebook_CustomersSpending.ipynb)]. These steps are followed to create the plot:  
- Read `FactSales` and `DimCustomers` parquet files from ADLS container.  
- Perform proper joins & aggregations (e.g., total spending per customer) and store results in a dataframe.  
- Calculate the median value of customer spending.  
- Generate the histogram of customer spendings using matplotlib.pyplot, with the median indicated by a dashed line.

<p align="center">
  <img src="https://github.com/user-attachments/assets/45d2c131-1e6c-467b-afca-6940970d5ba8" width="900">
</p>  

## 5Ô∏è‚É£ Connection to Power BI  

There are two ways of connecting **Azure Synapse** with **Power BI**:  

- Power BI Desktop ‚Üí Connect using server name + credentials.  
- Power BI Service (Online) ‚Üí Link Synapse workspace as a Linked Service for managed connectivity.  


‚ö†Ô∏è **Note:** In this project, we focused on Synapse itself and did not perform any dashboarding or reporting beyond connecting the Synapse SQL Pools to Power BI. For full dashboards, reporting, and visualization examples, please refer to the Related Projects section below.

  
# üí∞ Cost Considerations  
## üìã Overview
The approximate costs for Azure Synapse services are as follows:  

- **Serverless SQL Pool** ‚Üí ~$5/TB scanned (minimum 10 MB/query)  
- **Dedicated SQL Pool** ‚Üí Pay for DWUs (compute billed when active, storage billed separately)  
- **Apache Spark Pool** ‚Üí Pay for vCore-hours while running  

## üìã General Recommendations
- When using Serverless SQL Pool, **partition large files** to query only the required portion of data. This reduces the amount of data scanned and lowers costs.  
- Use **optimized file formats** such as Parquet or Delta instead of CSV for better performance and lower cost.  
- Use the **Cost Control** option to set daily/weekly/yearly limits when using Serverless SQL Pool, preventing accidental overspending due to errors or misconfigured queries.
- **Pause/stop provisioned compute resources** (Dedicated SQL Pool and Apache Spark) when idle to avoid unnecessary charges.  

## üí°Special Considerations when Connecting to Power BI 

Since cost models differ between **Serverless** and **Dedicated SQL Pools**, careful selection of connection/refresh strategy is important:  

- **Serverless SQL Pool**:  
  - Only **DirectQuery / Live Connection** is available (no import option).  
  - Manual refresh is recommended over automatic refresh to minimize the number of queries, reducing costs.  

- **Dedicated SQL Pool**:  
  - Both **Import** and **DirectQuery** options are available.  
  - Importing data is suitable for smaller datasets to improve performance.  
  - Always pause the dedicated SQL pool when not in use to avoid unnecessary compute charges.  


## üîë Technical Highlights  

This project demonstrates practical expertise in **Azure Synapse Analytics**, focusing on key compute options, query engines, and integration capabilities including:
- **Serverless SQL Pool**: ad-hoc queries on raw/processed files in ADLS Gen2; cost-efficient pay-per-query model.  
- **Dedicated SQL Pool**: high-performance analytics; internal tables with **Round-robin distribution** and **Clustered Columnstore Index (CCI)**; metadata primary keys for schema clarity.  
- **Apache Spark Pool**: flexible exploration, aggregation, and visualization using **PySpark**; supports integration with ADLS and Dedicated SQL Pool.  
- **Query & Analysis**: demonstrated multiple query approaches, external tables, and basic Synapse visualizations.  
- **Cost Awareness**: insights into serverless vs. dedicated vs. Spark cost models; recommended strategies for efficient usage.  
- **Integration Readiness**: connected Synapse SQL pools with Power BI for downstream reporting.  

‚ö†Ô∏è **Note:** Synapse also provides **pipelines** and **data flows** for ETL/ELT orchestration, but these are not covered here. For a detailed demonstration, see the **Azure Data Factory Project** via the link provided in the Related Projects section below.  

# üîÅ Related Projects  

These projects independently explore the same dataset with different objectives. Feel free to check them out:  
- ‚òÅÔ∏è [Azure Data Factory Project](https://github.com/Seyyed-Reza-Mashhadi/Azure-Data-Factory-Project): This project demonstrates a complete ETL pipeline and data orchestration using Azure Cloud Services, including Azure Data Factory (ADF), Azure Data Lake Storage Gen2 (ADLS), and Azure SQL Database. 
- üìä [Power BI Dashboard ‚Äì Grocery Sales](https://github.com/Seyyed-Reza-Mashhadi/Power-BI-Project_Grocery-Sales): An interactive dashboard that visually explores key trends including sales performance, product demand, customer spending metrics, employee highlights, and regional insights.
- üóÑÔ∏è [SQL Project ‚Äì Grocery Sales](https://github.com/Seyyed-Reza-Mashhadi/SQL-Project_Grocery-Sales): This companion project presents the PostgreSQL database design and extensive analytical SQL queries. It provides deep dives into revenue trends, customer segmentation, product performance, and employee effectiveness.


