<img width="1919" height="500" alt="banner" src="https://github.com/user-attachments/assets/8e49e8dd-418d-418c-bbbb-e9ff81318cdc" />

# ‚òÅÔ∏è Azure Synapse Analytics Project  

This project demonstrates how to perform analytics directly in **Azure Synapse Analytics**, leveraging different query engines and compute options. Using both the cleaned Parquet data produced by the Azure Data Factory project and the raw CSV files, the focus is on showcasing various Synapse tools for querying, modeling, and analyzing data, including:  
- **Serverless SQL Pools** (on-demand queries)  
- **Dedicated SQL Pools** (provisioned compute for structured analytics)  
- **Apache Spark Pools** (data exploration, transformation, and visualization)  

üîó **Dataset:** The data is available on [Kaggle](https://www.kaggle.com/datasets/155a87ba8d7e92c5896ddc7f3ca3e3fa9c799207ed8dbf9a1cedf2e2e03e3c14). Raw CSV files and processed Parquet outputs were already available in ADLS, prepared via pipelines from the [Azure Data Factory Project](https://github.com/Seyyed-Reza-Mashhadi/Azure-Data-Factory-Project).  


## üéØ Project Goals  

- Showcase how **different Synapse compute engines** (Serverless SQL, Dedicated SQL, Spark) can be applied to the same dataset  
- Demonstrate **end-to-end analytics workflow**: raw data exploration ‚Üí star schema modeling ‚Üí BI readiness  
- Highlight **cost-efficient data analysis strategies** in Synapse (when to use serverless vs. dedicated vs. Spark)  
- Enable **seamless Power BI integration** with Synapse for interactive dashboards  
- Provide a **reusable reference architecture** for modern data analytics on Azure  


# ‚öôÔ∏è Step-by-Step Implementation  

## 1Ô∏è‚É£ Azure Resources  

Provisioned within the same resource group:  
- **Azure Data Lake Storage Gen2** ‚Üí contains raw CSVs + cleaned Parquet outputs  
- **Azure Synapse Analytics** (workspace) with:  
  - **Serverless SQL Pool** ‚Üí default, on-demand queries  
  - **Dedicated SQL Pool** ‚Üí provisioned compute, relational schema  
  - **Apache Spark Pool** ‚Üí notebooks for data exploration & visualization  

<p align="center">
  <img src="https://github.com/user-attachments/assets/7ecaa9df-effd-4269-919a-13101035960a" width="350">
</p>  

## 2Ô∏è‚É£ Serverless SQL Pool  

Serverless SQL is Synapse‚Äôs **on-demand query engine** that lets you run SQL queries directly on files in **ADLS Gen2** without loading them into a database. It is ideal for **quick ad-hoc analysis** and validation of transformed outputs. Serverless SQL Pool features include:  
- No infrastructure provisioning required  
- Ideal for exploration & validation of raw/processed data  
- Can query diverse file formats (CSV, Parquet, JSON) directly  
- **Cost Model:** pay only per query (cost-efficient)  
  - ~$5 per TB of data processed  
  - Minimum 10 MB per query (even for smaller files ‚Üí still very cheap)  

### **Query Raw Data**

In this project, two example for raw data queries are illustrated using serverless SQL pool:  

- Queried Parquet files directly from ADLS Gen2 to get the top 5 most expensive "durable" products [[View SQL File](https://github.com/Seyyed-Reza-Mashhadi/Azure-Synapse-Analytics-Project/blob/main/SQL%20files/0_Direct_Query_csv.sql)]  

<p align="center">
  <img src="https://github.com/user-attachments/assets/fd8d104b-817b-420d-b15a-4832bf08480d" width="700">
</p>  

- Queried raw CSV directly from ADLS Gen2 to get the total number of cities with sales [[View SQL File](https://github.com/Seyyed-Reza-Mashhadi/Azure-Synapse-Analytics-Project/blob/main/SQL%20files/0_Direct_Query_parquet.sql)]    

<p align="center">
  <img src="https://github.com/user-attachments/assets/e27d7e2f-797e-468c-b93c-fa3edffbab8d" width="700">
</p>  


### **Query External Tables**

Serverless SQL pool also supports **external tables**, which are metadata definitions inside the database that point to files in ADLS.  
Key points about external tables:  
- The data itself remains in ADLS; the table only stores schema and location metadata  
- Allows querying raw or processed data as if it were a normal table in SQL  
- Useful for consistent queries, schema enforcement, and integration with views or downstream pipelines  
- Tables in serverless SQL pool are external by default (this is the opposite of dedicated SQL pool where tables are internal and data is physically stored in the database)  

Here is an example showing a query result using the serverless SQL pool for calculating the total revenue generated by different product categories [[View SQL File](https://github.com/Seyyed-Reza-Mashhadi/Azure-Synapse-Analytics-Project/blob/main/SQL%20files/1_Serverless_Query_1.sql)]. 

<p align="center">
  <img src="https://github.com/user-attachments/assets/5aa2a39d-8212-498b-98a2-2e4a25a70bb0" width="700">
</p>  

**‚ö°Quick Visualization in Synapse:**

Synapse provides some basic plotting options (bar charts, line charts, pie charts, etc.) that can be used for quick visualization of query results. These are useful for rapid validation and exploration, though more advanced analysis and dashboards are typically built in Power BI or using Apache Spark notebooks.


## 3Ô∏è‚É£ Dedicated SQL Pool  

Dedicated SQL Pool is **Synapse‚Äôs provisioned data warehouse** for structured, high-performance analytics using **T-SQL**. Dedicated SQL Pool features include:  
- Scalable data warehouse for BI workloads 
- Schema-aware integration with Power BI 
- High-performance, repeatable queries  
- Tables are **internal tables** (physically stored in the dedicated SQL database)  
- **Distribution Strategies:** Tables can use **hash, round-robin, or replicated distribution** to optimize parallel processing and query performance  
- **Cost Model:** Billed per hour based on **DWUs (Data Warehouse Units)**  
  - Compute billing stops when paused  
  - Storage billed at ~$0.12/GB/month  

In this project, processed **Parquet files** were loaded into the dedicated SQL database using **bulk load**. To optimize performance, the tables in the database were created with **Round-robin distribution** (ensuring even data spread across compute nodes), and a **Clustered Columnstore Index (CCI)** was applied by default, providing high compression and efficient analytical query execution on large datasets. After loading the data into internal tables, **Primary Keys** were introduced as **metadata** to provide schema clarity for downstream analysis (e.g., Power BI), without enforcing strict constraints, since Synapse Dedicated Pools are designed for **parallel, high-speed analytics**, not strict relational enforcement.

**Query Example**
- A query to get the top 5 best selling products (i.e., with highest generated revenue) [[View SQL File](https://github.com/Seyyed-Reza-Mashhadi/Azure-Synapse-Analytics-Project/blob/main/SQL%20files/2_Dedicated_Query_2.sql)]

<br>

<p align="center">
  <img src="https://github.com/user-attachments/assets/e1296dd9-35a1-464a-85e1-d78325dfec55" width="700">
</p>  


## 4Ô∏è‚É£ Apache Spark Pool  

Apache Spark in Synapse enables **data exploration, transformation, and visualization** using **Python / PySpark / ML libraries**. It is ideal for flexible analytics and exploratory data science workloads. Apache Spark features include:  
- Flexible data manipulation with **Python / PySpark / ML libraries**  
- Native integration with Synapse workspace for analytics & visualization  
- Suitable for **exploratory data science workloads**  
- Supports reading/writing from **ADLS Gen2**, **Dedicated SQL Pool**, or other external sources
- Spark notebooks can also be **integrated into Synapse pipelines** for scheduled or automated data processing, similar to Azure Data Factory pipelines.
- **Cost Model:** Billed per vCore-hour while the pool is running  
  - Must stop pool to avoid charges when not in use  
  - No persistent storage cost unless explicitly writing outputs  

**Example**
In this project, the histogram of customer total spendings is created using PySpark notebook [[View Notebook](https://github.com/Seyyed-Reza-Mashhadi/Azure-Synapse-Analytics-Project/blob/main/Notebook_CustomersSpending.ipynb)]. These steps are followed to create the plot:  
- Read `FactSales` and `DimCustomers` parquet files from ADLS container.  
- Perform proper joins & aggregations (e.g., total spending per customer) and store results in a dataframe.  
- Calculate the median value of customer spending.  
- Generate the histogram of customer spendings using matplotlib.pyplot, plotting the median as a line.

<p align="center">
  <img src="https://github.com/user-attachments/assets/45d2c131-1e6c-467b-afca-6940970d5ba8" width="900">
</p>  

## 5Ô∏è‚É£ Connection to Power BI  

There are two ways of connecting **Azure Synapse** with **Power BI**:  

- **Power BI Desktop** ‚Üí Connect using server name + credentials.  
- **Power BI Service (Online)** ‚Üí Link Synapse workspace as a **Linked Service** for managed connectivity.  


‚ö†Ô∏è **Note:** In this project, we focused on **Synapse itself** and did not perform any dashboarding or reporting beyond connecting the Synapse SQL Pools to Power BI. For full dashboards, reporting, and visualization examples, please refer to the **Related Projects** section below.

  
# üîÑ Synapse vs. ADF  

| Feature              | **Azure Data Factory (ADF)** | **Azure Synapse Analytics** |
|----------------------|-------------------------------|-----------------------------|
| **Primary Focus**    | ETL/ELT (data movement & transformation) | Analytics & data warehousing |
| **Data Handling**    | Ingest, clean, transform data | Query, model, visualize data |
| **Core Strengths**   | Pipelines, data orchestration | Serverless queries, Spark, Dedicated SQL pools |
| **Integration**      | Data prep for downstream use | BI-ready datasets & analysis |
| **Best For**         | Preparing structured data | Exploring & analyzing data |

üëâ In short: **ADF = Data Preparation**, **Synapse = Data Analysis**  

---

# üí∞ Cost Considerations  

The approximate costs for Azure Synapse services are as follows:  

- **Serverless SQL Pool** ‚Üí ~$5/TB scanned (minimum 10 MB/query)  
- **Dedicated SQL Pool** ‚Üí Pay for DWUs (compute billed when active, storage billed separately)  
- **Apache Spark Pool** ‚Üí Pay for vCore-hours while running  

Thus, here are some **General Recommendations**:
- When using Serverless SQL Pool, **partition large files** to query only the required portion of data. This reduces the amount of data scanned and lowers costs.  
- Use **optimized file formats** such as Parquet or Delta instead of CSV for better performance and lower cost.  
- Use the **Cost Control** option to set daily/weekly/yearly limits when using Serverless SQL Pool, preventing accidental overspending due to errors or misconfigured queries.
- **Pause/stop provisioned compute resources** (Dedicated SQL Pool and Apache Spark) when idle to avoid unnecessary charges.  

üí° **Special Considerations when Connecting to Power BI**  

Since cost models differ between **Serverless** and **Dedicated SQL Pools**, careful selection of connection/refresh strategy is important:  

- **Serverless SQL Pool**:  
  - Only DirectQuery / Live Connection is available (no import option).  
  - Manual refresh is recommended over automatic refresh to minimize the number of queries, reducing costs.  

- **Dedicated SQL Pool**:  
  - Both Import and DirectQuery options are available.  
  - Importing data is suitable for smaller datasets to improve performance.  
  - Always pause the dedicated SQL pool when not in use to avoid unnecessary compute charges.  



# üîë Key Features  

- Queried data directly from ADLS without ETL (Serverless SQL)  
- Built star schema in Dedicated SQL Pool  
- Integrated schema with Power BI for BI readiness  
- Used Apache Spark for aggregations & visualizations  
- Compared Synapse vs. ADF for clear separation of concerns  
- Highlighted cost efficiency across compute options  


# üîÅ Related Projects  

üìä **Azure Data Factory Project** ‚Üí End-to-end ETL pipelines & star schema prep  
üìä **SQL Project** ‚Üí PostgreSQL analytics on Grocery Sales dataset  
üìä **Power BI Dashboard** ‚Üí Interactive visual exploration of sales & customer insights  
